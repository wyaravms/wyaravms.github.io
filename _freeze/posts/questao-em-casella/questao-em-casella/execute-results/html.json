{
  "hash": "2f6b33c181ea2e9d77515f370cc37cff",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"EM Question\"\ntitle-block-banner: true\nformat:\n  html:\n    page-layout: article\n    html-math-method: mathjax\n    number-sections: true  # ativa a numeração das seções e equações\nauthor: \"Wyara Moura\"\ndate: \"2024-12-26\"\ncategories: [inference, probability, EM, MLE, casella and berger]\n---\n\n\n::: hidden\n\\usepackage{amsfonts}\n\\usepackage{dsfont}\n\\usepackage{listings}\n\\usepackage{float}\n\\renewcommand{\\baselinestretch}{1.2}\n\\renewcommand{\\arraystretch}{1.2}\n:::\n\n# Question 7.29 (Casella and Berger, 2002)\n\nAn alternative to the model in Example 7.2.17 (Casella and Berger, 2002) is the following: we observe $(Y_{i}, X_{i})$, $i = 1, 2, \\cdots, n$, where $Y_{i} \\sim \\mathcal{P}\\mathrm{oisson}(m\\beta\\tau_{i})$ and $(X_{1}, \\cdots, X_{n}) \\sim \\mathcal{M}\\mathrm{ultinomial}(m; \\boldsymbol{\\tau})$, with $\\boldsymbol{\\tau} = (\\tau_{1}, \\cdots, \\tau_{n})$ such that $\\sum_{i=1}^{n} \\tau_{i} = 1$. Thus, here, for instance, we assume that population counts are multinomial allocations instead of Poisson counts. (Consider $m = \\sum_{i=1}^{n} x_{i}$ as known.)\n\n(a) Show that the joint density of $\\textbf{Y} = (Y_{1}, \\cdots, Y_{n})$ and $\\textbf{X} = (X_{1}, \\cdots, X_{n})$ is\n\n\\begin{equation*}\n\\begin{array}{rclll}\nf(\\textbf{y}, \\textbf{x}\\mid \\beta, \\boldsymbol{\\tau}) & = & \\displaystyle\\prod_{i=1}^{n}\\dfrac{e^{-m\\beta\\tau_{i}}(m\\beta\\tau_{i})^{y_{i}}}{y_{i}!}m!\\dfrac{\\tau_{i}^{x_{i}}}{x_{i}!}.\n\\end{array}\n\\end{equation*} \n\n## Solution:\n\nAssuming that $X_{i}$ and $Y_{i}$ are mutually independent for all $i = 1, \\cdots, n$, the joint probability function is given by the product of the probability functions, that is,\n\n\\begin{equation*}\n\\begin{array}{rclll}\nf(\\textbf{y}, \\textbf{x}\\mid \\beta, \\boldsymbol{\\tau}) & = & \\left[ \\displaystyle\\prod_{i=1}^{n}f(y_{i} \\mid \\beta,  \\boldsymbol{\\tau})\\right] f(\\textbf{x}\\mid \\boldsymbol{\\tau})\\\\[14pt]\n\n& = & \\left[\\displaystyle\\prod_{i=1}^{n}\\dfrac{e^{-m\\beta\\tau_{i}}(m\\beta\\tau_{i})^{y_{i}}}{y_{i}!}\\right]m!\\dfrac{\\tau_{1}^{x_{1}} \\cdots \\tau_{n}^{x_{n}}}{x_{1}!\\cdots x_{n}!}. \\\\[14pt]\n\n& = & \\displaystyle\\prod_{i=1}^{n}\\dfrac{e^{-m\\beta\\tau_{i}}(m\\beta\\tau_{i})^{y_{i}}}{y_{i}!}m!\\dfrac{\\tau_{i}^{x_{i}}}{x_{i}!}. \\\\[8pt]\n\\end{array}\n\\end{equation*} \n\n(b) If the complete data are observed, show that the maximum likelihood estimators (MLEs) are given by\n\n\\begin{equation*}\n\\begin{array}{rclll}\n\\hat{\\beta} & = & \\dfrac{\\sum_{i=1}^{n}y_{i}}{\\sum_{i=1}^{n}x_{i}} \\qquad \\mbox{e} \\qquad \\hat{\\tau}_{j} & = & \\dfrac{x_{j} + y_{j}}{\\sum_{i=1}^{n}x_{i} + y_{i}}, \\qquad j=1, 2, \\cdots, n.\n\\end{array}\n\\end{equation*} \n\n## Solution:\n\nInitially, the log-likelihood function is given by:\n\n\\begin{equation}\n\\begin{array}{rclll}\n\\log [f(\\textbf{y}, \\textbf{x}\\mid \\beta, \\boldsymbol{\\tau})] & = & \\log\\left[\\displaystyle\\prod_{i=1}^{n}\\dfrac{e^{-m\\beta\\tau_{i}}(m\\beta\\tau_{i})^{y_{i}}}{y_{i}!}m!\\dfrac{\\tau_{i}^{x_{i}}}{x_{i}!}\\right] \\\\[10pt]\n\n& = & \\displaystyle\\sum_{i=1}^{n}\\log\\left[\\dfrac{e^{-m\\beta\\tau_{i}}(m\\beta\\tau_{i})^{y_{i}}}{y_{i}!}m!\\dfrac{\\tau_{i}^{x_{i}}}{x_{i}!}\\right] \\\\[8pt]\n\n& = & \\displaystyle\\sum_{i=1}^{n}\\left[-m\\beta\\tau_{i} + \\log(m\\beta\\tau_{i})^{y_{i}}-\\log{(y_{i}!)} + \\log (m!) + \\log(\\tau_{i}^{x_{i}}) - \\log{(x_{i}!)}\\right] \\\\[8pt]\n\n& = & \\displaystyle\\sum_{i=1}^{n}\\left[-m\\beta\\tau_{i} + {y_{i}}\\log(m\\beta\\tau_{i})-\\log{(y_{i}!)} + \\log (m!) + {x_{i}}\\log(\\tau_{i}) - \\log{(x_{i}!)}\\right] \\\\[10pt]\n\\end{array}\n\\label{equation1}\n\\end{equation} \n\nWhere the first partial derivatives with respect to $\\beta$ and $\\tau_{j}$ are given by the following expressions.\n\n\\begin{itemize}\n    \\item Para $\\beta$:\n\\end{itemize}\n\\begin{equation*}\n\\begin{array}{rclll}\n\\dfrac{\\partial}{\\partial\\beta}\\log[f(\\textbf{y}, \\textbf{x}\\mid \\beta, \\boldsymbol{\\tau})] & = & \\dfrac{\\partial}{\\partial\\beta}\\displaystyle\\sum_{i=1}^{n}\\left[-m\\beta\\tau_{i} + {y_{i}}\\log(m\\beta\\tau_{i})-\\log{(y_{i}!x_{i}!)} + \\log (m!) + {x_{i}}\\log(\\tau_{i})\\right] \\\\[10pt]\n\n& = & \\displaystyle\\sum_{i=1}^{n}\\dfrac{\\partial}{\\partial\\beta}\\left[-m\\beta\\tau_{i} + {y_{i}}\\log(m\\beta\\tau_{i})-\\log{(y_{i}!x_{i}!)} + \\log (m!) + {x_{i}}\\log(\\tau_{i})\\right] \\\\[10pt]\n\n& = & \\displaystyle\\sum_{i=1}^{n}\\left[-m\\tau_{i} + {y_{i}}\\dfrac{m\\tau_{i}}{m\\beta\\tau_{i}}\\right] \\\\[10pt]\n\n& = & -m\\displaystyle\\sum_{i=1}^{n}\\tau_{i} + \\dfrac{1}{\\beta}\\displaystyle\\sum_{i=1}^{n}{y_{i}}\\\\[12pt]\n\n\\end{array}\n\\end{equation*} \n\nWhere, as per the initial hypothesis, $m = \\sum_{i=1}^{n} x_{i}$ and $\\sum_{i=1}^{n} \\tau_{i} = 1$, then:\n\n\\begin{equation*}\n\\begin{array}{rclll}\n\\dfrac{\\partial}{\\partial\\beta}\\log[f(\\textbf{y}, \\textbf{x}\\mid \\beta, \\boldsymbol{\\tau})] \n\n& = & -\\displaystyle\\sum_{i=1}^{n}x_{i} + \\dfrac{1}{\\beta}\\displaystyle\\sum_{i=1}^{n}{y_{i}}\\\\[10pt]\n\n\\end{array}\n\\end{equation*}\n\n\\begin{itemize}\n    \\item For $\\tau_{j}$:\n\\end{itemize}\n\\begin{equation*}\n\\begin{array}{rclll}\n\\dfrac{\\partial}{\\partial\\tau_{j}}\\log[f(y_{j}, x_{j}\\mid \\beta, \\tau_{j})] & = & \\dfrac{\\partial}{\\partial\\tau_{j}}\\left[-m\\beta\\tau_{j} + {y_{j}}\\log(m\\beta\\tau_{j})-\\log{(y_{j}!x_{j}!)} + \\log (m!) + {x_{j}}\\log(\\tau_{j})\\right] \\\\[6pt]\n\n& = & -m\\beta + {y_{j}}\\dfrac{m\\beta}{m\\beta\\tau_{j}} + {x_{j}}\\dfrac{1}{\\tau_{j}}  \\\\[8pt]\n\n& = & -m\\beta + {y_{j}}\\dfrac{1}{\\tau_{j}} + {x_{j}}\\dfrac{1}{\\tau_{j}} \\\\[8pt]\n& = & -m\\beta + \\dfrac{y_{j} + x_{j}}{\\tau_{j}} \\\\[8pt]\n\\end{array}\n\\end{equation*} \n\n\nNow, by setting these partial derivatives equal to 0, we will obtain as a solution the following maximum likelihood estimators:\n\n\\begin{itemize}\n    \\item Para $\\beta$:\n\\end{itemize}\n\\begin{equation*}\n\\begin{array}{rclllll}\n\\dfrac{\\partial}{\\partial\\beta}\\log[f(\\textbf{y}, \\textbf{x}\\mid \\beta, \\boldsymbol{\\tau})] & = & 0 \\\\[8pt] \n -\\displaystyle\\sum_{i=1}^{n}x_{i} + \\dfrac{1}{\\hat{\\beta}}\\displaystyle\\sum_{i=1}^{n}{y_{i}} & = & 0\\\\\n\\dfrac{1}{\\hat{\\beta}}\\displaystyle\\sum_{i=1}^{n}{y_{i}} & = & \\displaystyle\\sum_{i=1}^{n}x_{i} & \\Longrightarrow &\n\\hat{\\beta}  & = & \\dfrac{\\displaystyle\\sum_{i=1}^{n}{y_{i}}}{\\displaystyle\\sum_{i=1}^{n}x_{i}}\\\\[6pt]\n\\end{array}\n\\end{equation*} \n\n\\begin{itemize}\n    \\item For $\\tau_{j}$:\n\\end{itemize}\n\n\\begin{equation}\n\\begin{array}{rclll}\n\\dfrac{\\partial}{\\partial\\tau_{j}}\\log[f(y_{j}, x_{j}\\mid \\beta, \\boldsymbol{\\tau})] & = & 0 \\\\[6pt]\n\n-m\\hat{\\beta} + \\dfrac{y_{j} + x_{j}}{\\hat{\\tau}_{j}} & = & 0\\\\[8pt]\n\\dfrac{y_{j} + x_{j}}{\\hat{\\tau}_{j}} & = & m\\hat{\\beta}\\\\[6pt]\n\n\\hat{\\tau}_{j} & = & \\dfrac{y_{j} + x_{j}}{m\\hat{\\beta}}\\\\[8pt]\n\\end{array}\n\\label{tauemv}\n\\end{equation}\n\n\nWhere, considering this previous result and starting from the initial hypothesis that $\\sum_{i=1}^{n} \\tau_{i} = 1$, we have:\n\n\\begin{equation*}\n\\begin{array}{rclll}\n%0 & = & -m\\beta + \\dfrac{y_{j} + x_{j}}{\\tau_{j}} \\\\[8pt]\n%\\tau_{j} & = & \\dfrac{y_{j} + x_{j}}{m\\beta} \\\\[8pt]\n\\displaystyle\\sum_{i=1}^{n}\\tau_{i} & = & \\displaystyle\\sum_{i=1}^{n}\\left(\\dfrac{y_{i} + x_{i}}{m\\beta}\\right)\\\\[8pt]\n1 & = & \\dfrac{1}{m\\beta}\\displaystyle\\sum_{i=1}^{n}\\left(y_{i} + x_{i}\\right)\\\\[8pt]\nm\\beta & = & \\displaystyle\\sum_{i=1}^{n}\\left(y_{i} + x_{i}\\right)\\\\[8pt]\n\\end{array}\n\\end{equation*}\n\nThus, the estimator for $\\hat{\\tau}_{j}$ will be given by:\n\n\\begin{equation*}\n\\begin{array}{rclll}\n\\hat{\\tau}_{j} & = & \\dfrac{y_{j} + x_{j}}{\\displaystyle\\sum_{i=1}^{n}\\left(y_{i} + x_{i}\\right)}\\\\[8pt]\n\\end{array}\n\\end{equation*} \n\nQ.E.D. \n\n(c) Suppose that $x_{i}$ is missing. Use the fact that $X_{1} \\sim \\mathcal{B}\\mathrm{inomial}(m, \\tau_{1})$ to calculate the expected log-likelihood of the complete data. Show that the EM sequence is given by\n\n\\begin{equation*}\n\\begin{array}{rclll}\n\\hat{\\beta}^{(r+1)} & = & \\dfrac{\\sum_{i=1}^{n}y_{i}}{m\\hat{\\tau}_{1}^{(r)} + \\sum_{i=2}^{n}x_{i}} \\qquad \\mbox{e}  & \\\\[18pt]\n\\qquad \\hat{\\tau}_{j}^{(r+1)} & = & \\dfrac{x_{j} + y_{j}}{m\\hat{\\tau}_{1}^{(r)} + \\sum_{i=2}^{n}x_{i} + \\sum_{i=1}^{n}y_{i}}, \\qquad j=1, 2, \\cdots, n.\\\\[8pt]\n\\end{array}\n\\end{equation*} \n\n\n## Solution:\n\nLet the vector $(\\textbf{x}_{(-1)}, \\textbf{y}) = (y_{1}, (x_{2}, y_{2}), \\cdots, (x_{n}, y_{n}))$, which denotes the incomplete data. The expected log-likelihood of the complete data, using the information provided in the problem that $X_{1} \\sim \\mathcal{B}\\mathrm{inomial}(m, \\tau_{1})$, will be:\n\n\\begin{equation*}\n\\begin{array}{lclll}\n \\mathbb{E}\\left[ \\log[f(\\textbf{y}, \\textbf{x}\\mid \\beta, \\boldsymbol{\\tau})] \\mid \\tau_{1}^{(r)}, (\\textbf{x}_{(-1)}, \\textbf{y}) \\right] \\; = \\\\[14pt]\n = \\; \\displaystyle\\sum_{x_{1}=0}^{m} \\log\\left[\\displaystyle\\prod_{i=1}^{n}\\dfrac{e^{-m\\beta\\tau_{i}}(m\\beta\\tau_{i})^{y_{i}}}{y_{i}!}m!\\dfrac{\\tau_{i}^{x_{i}}}{x_{i}!}\\right] {m\\choose x_{1}} \\left(\\tau_{1}^{(r)}\\right)^{x_{1}}\\left(1-\\tau_{1}^{(r)}\\right)^{m-x_{1}}  \\\\[18pt]\n\\end{array}\n\\end{equation*} \n\n\\begin{equation*}\n\\begin{array}{lllll}\n = & \\displaystyle\\sum_{x_{1}=0}^{m} \\displaystyle\\sum_{i=1}^{n}\\log\\left[\\dfrac{e^{-m\\beta\\tau_{i}}(m\\beta\\tau_{i})^{y_{i}}}{y_{i}!}m!\\dfrac{\\tau_{i}^{x_{i}}}{x_{i}!}\\right] {m\\choose x_{1}} \\left(\\tau_{1}^{(r)}\\right)^{x_{1}}\\left(1-\\tau_{1}^{(r)}\\right)^{m-x_{1}}  \\\\[18pt]\n = & \\displaystyle\\sum_{i=1}^{n}\\left[-m\\beta\\tau_{i} + {y_{i}}[\\log(m\\beta\\tau_{i})]-\\log{(y_{i}!)} + \\log(m!)\\right] \\displaystyle\\sum_{x_{1}=0}^{m}{m\\choose x_{1}} \\left(\\tau_{1}^{(r)}\\right)^{x_{1}}\\left(1-\\tau_{1}^{(r)}\\right)^{m-x_{1}} \\; + \\\\[12pt]  \n& + \\; \\displaystyle\\sum_{i=2}^{n}\\left[{x_{i}}\\log(\\tau_{i}) -\\log{(x_{i}!)}\\right]\\displaystyle\\sum_{x_{1}=0}^{m}{m\\choose x_{1}} \\left(\\tau_{1}^{(r)}\\right)^{x_{1}}\\left(1-\\tau_{1}^{(r)}\\right)^{m-x_{1}} \\; +\n\\\\[12pt] \n& + \\displaystyle\\sum_{x_{1}=0}^{m}\\left[{x_{1}}\\log(\\tau_{1}) -\\log{(x_{1}!)} \\right]{m\\choose x_{1}} \\left(\\tau_{1}^{(r)}\\right)^{x_{1}}\\left(1-\\tau_{1}^{(r)}\\right)^{m-x_{1}}  \\\\[12pt]  \n\n= & \\displaystyle\\sum_{i=1}^{n}\\left[-m\\beta\\tau_{i} + {y_{i}}[\\log(m\\beta\\tau_{i})]-\\log{(y_{i}!)} + \\log(m!)\\right] \\; + \\\\[12pt]  \n& + \\; \\displaystyle\\sum_{i=2}^{n}\\left[{x_{i}}\\log(\\tau_{i}) -\\log{(x_{i}!)}\\right] + \\displaystyle\\sum_{x_{1}=0}^{m}\\left[{x_{1}}\\log(\\tau_{1}) - \\log{(x_{1}!)}\\right]{m\\choose x_{1}} \\left(\\tau_{1}^{(r)}\\right)^{x_{1}}\\left(1-\\tau_{1}^{(r)}\\right)^{m-x_{1}}  \\\\[12pt]\n\n= & \\displaystyle\\sum_{i=1}^{n}\\left[-m\\beta\\tau_{i} + {y_{i}}[\\log(m\\beta\\tau_{i})]-\\log{(y_{i}!)} + \\log(m!)\\right] \\; + \\\\[12pt]  \n& + \\; \\displaystyle\\sum_{i=2}^{n}\\left[{x_{i}}\\log(\\tau_{i}) -\\log{(x_{i}!)}\\right] + \\displaystyle\\sum_{x_{1}=0}^{m}\\left[{x_{1}}\\log(\\tau_{1})\\right]{m\\choose x_{1}} \\left(\\tau_{1}^{(r)}\\right)^{x_{1}}\\left(1-\\tau_{1}^{(r)}\\right)^{m-x_{1}} \\; -  \\\\[12pt]  \n\n& - \\;\\displaystyle\\sum_{x_{1}=0}^{m} \\left[\\log{(x_{1}!)} \\right]{m\\choose x_{1}} \\left(\\tau_{1}^{(r)}\\right)^{x_{1}}\\left(1-\\tau_{1}^{(r)}\\right)^{m-x_{1}}  \\\\[12pt]\n\n= & \\displaystyle\\sum_{i=1}^{n}\\left[-m\\beta\\tau_{i} + {y_{i}}[\\log(m) + \\log(\\beta) + \\log(\\tau_{i})]-\\log{(y_{i}!)} + \\log(m!)\\right] \\; + \\\\[12pt]  \n& + \\; \\displaystyle\\sum_{i=2}^{n}\\left[{x_{i}}\\log(\\tau_{i}) -\\log{(x_{i}!)}\\right] + \\log(\\tau_{1})\\displaystyle\\sum_{x_{1}=0}^{m}\\left[{x_{1}}\\right]{m\\choose x_{1}} \\left(\\tau_{1}^{(r)}\\right)^{x_{1}}\\left(1-\\tau_{1}^{(r)}\\right)^{m-x_{1}} \\; -  \\\\[12pt]  \n\n& - \\;\\displaystyle\\sum_{x_{1}=0}^{m} \\left[\\log{(x_{1}!)} \\right]{m\\choose x_{1}} \\left(\\tau_{1}^{(r)}\\right)^{x_{1}}\\left(1-\\tau_{1}^{(r)}\\right)^{m-x_{1}}  \\\\[12pt]\n\n= & \\left( \\displaystyle\\sum_{i=1}^{n}\\left[-m\\beta\\tau_{i} + {y_{i}}[\\log(m) + \\log(\\beta) + \\log(\\tau_{i})]-\\log{(y_{i}!)} + \\log(m!)\\right] \\; + \\right. \\\\[12pt]  \n& + \\;\\left. \\displaystyle\\sum_{i=2}^{n}\\left[{x_{i}}\\log(\\tau_{i}) -\\log{(x_{i}!)}\\right] + \\log(\\tau_{1})\\mathbb{E}\\left( X_{1}\\mid \\tau_{1}^{(r)}\\right)\\right) \\; -  \\\\[12pt]  \n\n& - \\;\\left( \\displaystyle\\sum_{x_{1}=0}^{m} \\left[\\log{(x_{1}!)} \\right]{m\\choose x_{1}} \\left(\\tau_{1}^{(r)}\\right)^{x_{1}}\\left(1-\\tau_{1}^{(r)}\\right)^{m-x_{1}} \\right) \\\\[12pt]\n\\end{array}\n\\end{equation*} \n\nSince we are calculating this expected value of the log-likelihood with the purpose of maximizing with respect to $\\beta$ and $\\boldsymbol{\\tau} = (\\tau_{1}, \\cdots, \\tau_{n})$, we can ignore the terms in the second set of parentheses. Therefore, we only need to maximize the terms in the first set of parentheses, where we observe that the expected log-likelihood of the complete data is equivalent to the original log-likelihood of the complete data found in part (b), with the exception that $x_{1}$ is replaced by $\\mathbb{E}\\left( X_{1} \\mid \\tau_{1}^{(r)} \\right) = m\\tau_{1}^{(r)}$, as per the initial hypothesis. Thus,\n\n\n\\begin{equation*}\n\\begin{array}{lclll}\n \\mathbb{E}\\left[ \\log[f(\\textbf{y}, \\textbf{x}\\mid \\beta, \\boldsymbol{\\tau})] \\mid \\tau_{1}^{(r)}, (\\textbf{x}_{(-1)}, \\textbf{y}) \\right] \\; = \\\\[10pt]\n = \\; \\left( \\displaystyle\\sum_{i=1}^{n}\\left[-m\\beta\\tau_{i} + {y_{i}}[\\log(m) + \\log(\\beta) + \\log(\\tau_{i})]-\\log{(y_{i}!)} + \\log(m!)\\right] \\; + \\right. \\\\[12pt]  \n\\; + \\;\\left. \\displaystyle\\sum_{i=2}^{n}\\left[{x_{i}}\\log(\\tau_{i}) -\\log{(x_{i}!)}\\right] + \\log(\\tau_{1})m\\tau_{1}^{(r)}\\right) - \\left( \\displaystyle\\sum_{x_{1}=0}^{m} \\left[\\log{(x_{1}!)} \\right]{m\\choose x_{1}} \\left(\\tau_{1}^{(r)}\\right)^{x_{1}}\\left(1-\\tau_{1}^{(r)}\\right)^{m-x_{1}} \\right) \\\\[12pt]  \n\n\\end{array}\n\\end{equation*} \n\nTherefore, in the $r$-th iteration, the maximum likelihood estimators (MLEs) are simply a variation of the MLEs found in part (b), replacing $x_{1}$ by $\\mathbb{E}\\left( X_{1} \\mid \\tau_{1}^{(r)} \\right) = m\\tau_{1}^{(r)}$, that is,\n\n\\begin{equation*}\n\\begin{array}{rclllll}\n\\hat{\\beta}^{(r+1)} & = & \\dfrac{\\sum_{i=1}^{n}y_{i}}{x_{1}+ \\sum_{i=2}^{n}x_{i}} & = & \\dfrac{\\sum_{i=1}^{n}y_{i}}{m\\hat{\\tau}_{1}^{(r)} + \\sum_{i=2}^{n}x_{i}} \\\\[18pt]\n\\qquad \\hat{\\tau}_{j}^{(r+1)} & = & \\dfrac{x_{j} + y_{j}}{x_{1} + \\sum_{i=2}^{n}x_{i} + \\sum_{i=1}^{n}y_{i}} & = & \\dfrac{x_{j} + y_{j}}{m\\hat{\\tau}_{1}^{(r)} + \\sum_{i=2}^{n}x_{i} + \\sum_{i=1}^{n}y_{i}}\\\\[10pt]\n\\end{array}\n\\end{equation*} \n\nThus, we have that the EM sequence is given by:\n\n\\begin{equation*}\n\\begin{array}{rclllll}\n\\hat{\\beta}^{(r+1)} & = & \\dfrac{\\sum_{i=1}^{n}y_{i}}{m\\hat{\\tau}_{1}^{(r)} + \\sum_{i=2}^{n}x_{i}},   & \\qquad \\hat{\\tau}_{1}^{(r+1)} & = & \\dfrac{m\\hat{\\tau}_{1}^{(r)} + y_{1}}{m\\hat{\\tau}_{1}^{(r)} + \\sum_{i=2}^{n}x_{i} + \\sum_{i=1}^{n}y_{i}} \\\\[18pt]\n\\end{array}\n\\end{equation*} \n\\begin{equation*}\n\\begin{array}{rclllll}\n\\qquad \\hat{\\tau}_{j}^{(r+1)} & = & \\dfrac{x_{j} + y_{j}}{m\\hat{\\tau}_{1}^{(r)} + \\sum_{i=2}^{n}x_{i} + \\sum_{i=1}^{n}y_{i}}, \\qquad j=2, \\cdots, n.\\\\[15pt]\n\\end{array}\n\\end{equation*} \n\nQ.E.D.\n\n(d) Use this model to find the maximum likelihood estimators (MLEs) for the data from Exercise 7.28 (Casella and Berger, 2002), first assuming that you have all the data and then assuming that $x_{1} = 3540$ is missing.\n\n\n## Solution:\n\nTo find the maximum likelihood estimators (MLEs) for $\\hat{\\beta}$ and $\\boldsymbol{\\hat{\\tau}}$ in this question, we will use the programming language \\texttt{R} (R Core Team, 2023) to implement the algorithm.\n\nThus, the MLEs found using the complete data were the following:\n\\begin{equation*}\n\\begin{array}{rclll}\n\\hat{\\beta} & = & 0.0008413892 \\\\\n\n\\hat{\\boldsymbol{\\tau}} & = & (0.06337310, 0.06374873, 0.06689681, 0.04981487, 0.04604075, 0.04883109, \\\\\n&& 0.07072460, 0.01776164, 0.03416388, 0.01695673, 0.02098127, 0.01878119, \\\\\n&& 0.05621836, 0.09818091, 0.09945087, 0.05267677, 0.08896918, 0.08642925)\\\\\n\\end{array}\n\\end{equation*}\n\nNow, the MLEs found using the incomplete data, and considering $m = \\sum_{i} x_{i}$, were:\n\n\\begin{equation*}\n\\begin{array}{rclll}\n\\hat{\\beta} & = & 0.0008415534 \\\\\n\n\\hat{\\boldsymbol{\\tau}} & = & (0.06319044, 0.06376116, 0.06690986, 0.04982459, 0.04604973, 0.04884062, \\\\\n&& 0.07073839, 0.01776510, 0.03417054, 0.01696004, 0.02098536, 0.01878485, \\\\\n&& 0.05622933, 0.09820005, 0.09947027, 0.05268704, 0.08898653, 0.08644610)\\\\\n\\end{array}\n\\end{equation*}\n\nIt is worth noting that in the \\texttt{R} code given below, the value of $\\tau_{1}$ used in the second algorithm for the incomplete data is initialized with the $\\tau_{1}$ found in the result of the first algorithm with the complete data. The \\texttt{R} code is as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# emvs para os dados completos\nxdata <- c(\n  3540, 3560, 3739, 2784, 2571, 2729, 3952, 993, 1908, 948, 1172,\n  1047, 3138, 5485, 5554, 2943, 4969, 4828\n)\nydata <- c(3, 4, 1, 1, 3, 1, 2, 0, 2, 0, 1, 3, 5, 4, 6, 2, 5, 4)\n\nfor (j in 1:500) {\n  beta <- sum(ydata) / sum(xdata)\n  tau <- c((xdata + ydata) / (sum(xdata) + sum(ydata)))\n}\nbeta\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0008413892\n```\n\n\n:::\n\n```{.r .cell-code}\ntau\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 0.06337310 0.06374873 0.06689681 0.04981487 0.04604075 0.04883109\n [7] 0.07072460 0.01776164 0.03416388 0.01695673 0.02098127 0.01878119\n[13] 0.05621836 0.09818091 0.09945087 0.05267677 0.08896918 0.08642925\n```\n\n\n:::\n\n```{.r .cell-code}\n# emvs para os dados imcompletos\nxdatam <- c(\n  3560, 3739, 2784, 2571, 2729, 3952, 993, 1908, 948, 1172,\n  1047, 3138, 5485, 5554, 2943, 4969, 4828\n)\nydata <- c(3, 4, 1, 1, 3, 1, 2, 0, 2, 0, 1, 3, 5, 4, 6, 2, 5, 4)\nxdata <- c(mean(xdatam), xdatam)\ni <- 0\n\nfor (j in 1:500) {\n  xdata <- c(sum(xdata) * tau[1], xdatam)\n  beta <- sum(ydata) / sum(xdata)\n  tau <- c((xdata + ydata) / (sum(xdata) + sum(ydata)))\n  i <- i + 1\n}\nbeta\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0008415534\n```\n\n\n:::\n\n```{.r .cell-code}\ntau\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 0.06319044 0.06376116 0.06690986 0.04982459 0.04604973 0.04884062\n [7] 0.07073839 0.01776510 0.03417054 0.01696004 0.02098536 0.01878485\n[13] 0.05622933 0.09820005 0.09947027 0.05268704 0.08898653 0.08644610\n```\n\n\n:::\n:::\n\n\n## Referências\n\n\\noindent\nCASELLA, G.; BERGER, R. L. Statistical Inference. 2. ed. Pacific Grove, CA: Duxbury Press, 2002. \n\n\\noindent\nR CORE TEAM. R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria, 2023. Disponível em: https://www.R-project.org/. \n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}